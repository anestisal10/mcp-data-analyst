# ğŸ“Š Data Analyst MCP Server

An **MCP (Model Context Protocol)** server that turns your LLM into a data analyst. Connect it to a database or data file and get:

- **Schema context** â€” the LLM automatically sees your tables and columns
- **Safe SQL queries** â€” ask questions in natural language, get SQL results
- **Data visualization** â€” generate charts and summary statistics

## ğŸ—ï¸ Architecture

```mermaid
graph LR
    Client["MCP Client<br/>(Claude / Gemini)"]
    Server["MCP Server<br/>(FastMCP)"]
    Tools["Tools"]
    Resources["Resources"]
    DB["SQLite / CSV / Excel"]

    Client <-->|"MCP Protocol<br/>(stdio)"| Server
    Server --> Tools
    Server --> Resources

    subgraph Tools
        direction TB
        T1["get_schema"]
        T2["run_read_only_query"]
        T3["visualize_data"]
    end

    subgraph Resources
        R1["data://schema"]
    end

    Tools --> DB
    Resources --> DB
```

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ—„ï¸ Multi-source | SQLite databases, CSV files, Excel files |
| ğŸ”’ Read-only safety | Only `SELECT` queries allowed â€” no accidental data modification |
| ğŸ“‹ Schema as Resource | LLM sees table structure automatically |
| ğŸ“Š Visualization | Bar, line, scatter, pie, and histogram charts |
| ğŸ“ˆ Statistics | Automatic summary statistics with every visualization |
| ğŸ“ Structured Logging | JSON-formatted logs for every operation |
| ğŸ³ Docker | One-command setup with `docker-compose` |
| âœ… CI/CD | GitHub Actions (lint + test + Docker build) |

## ğŸ“ Project Structure

```
MCP_Data_Analyst/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.py            # MCP server entry point
â”‚   â”œâ”€â”€ db.py                # Database engine (SQLite/CSV/Excel)
â”‚   â”œâ”€â”€ resources.py         # MCP resources (schema context)
â”‚   â”œâ”€â”€ logging_config.py    # Structured JSON logging setup
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ query.py         # Read-only SQL query tool
â”‚       â””â”€â”€ visualize.py     # Chart generation tool
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py          # Shared fixtures (in-memory DB)
â”‚   â”œâ”€â”€ test_db.py           # Schema, validation, execution tests
â”‚   â”œâ”€â”€ test_tools.py        # Query + visualization tool tests
â”‚   â””â”€â”€ test_resources.py    # Resource formatting tests
â”œâ”€â”€ data/                    # Data files directory
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yml               # GitHub Actions CI pipeline
â”œâ”€â”€ Dockerfile               # Multi-stage Docker build
â”œâ”€â”€ docker-compose.yml       # One-command Docker setup
â”œâ”€â”€ create_sample_db.py      # Generates demo database
â”œâ”€â”€ requirements.txt
â””â”€â”€ pyproject.toml
```

## ğŸš€ Quick Start

### Option 1: Docker (recommended)

```bash
docker-compose up --build
```

The container ships with a pre-built sample database. To use your own data, place it in the `data/` directory and set `DATA_SOURCE` in `docker-compose.yml`.

### Option 2: Local

```bash
# Install dependencies
pip install -r requirements.txt

# Create sample database (optional)
python create_sample_db.py

# Run the server
python -m src.server
```

## âš™ï¸ Configuration

Set the `DATA_SOURCE` environment variable to point to your data file:

| File Type | Extensions | Notes |
|-----------|-----------|-------|
| SQLite | `.db`, `.sqlite`, `.sqlite3` | Direct connection |
| CSV | `.csv` | Loaded into in-memory SQLite |
| Excel | `.xlsx`, `.xls` | Each sheet becomes a table |

If `DATA_SOURCE` is not set, the server uses `data/sample.db`.

## ğŸ”Œ MCP Client Setup

### Claude Desktop

Add to your `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "data-analyst": {
      "command": "python",
      "args": ["-m", "src.server"],
      "cwd": "/path/to/MCP_Data_Analyst",
      "env": {
        "DATA_SOURCE": "data/sample.db"
      }
    }
  }
}
```

### Gemini CLI

Add to your `settings.json`:

```json
{
  "mcpServers": {
    "data-analyst": {
      "command": "python",
      "args": ["-m", "src.server"],
      "cwd": "/path/to/MCP_Data_Analyst"
    }
  }
}
```

## ğŸ› ï¸ Available Tools

### `get_schema`
Get the database schema â€” lists all tables with their columns and types. Call this first to understand what data is available.

**Example prompt:** *"What tables are available in the database?"*

### `run_read_only_query`
Execute safe SQL queries. Results returned as markdown tables (max 100 rows).

**Example prompt:** *"Show me the top 5 highest-paid employees"*

### `visualize_data`
Generate charts from SQL query results.

**Parameters:**
- `sql` â€” SQL SELECT query
- `chart_type` â€” `bar`, `line`, `scatter`, `pie`, `hist`
- `x_column` / `y_column` â€” axis columns (auto-detected if omitted)
- `title` â€” chart title

**Example prompt:** *"Create a bar chart of total sales by region"*

## ğŸ“š Resources

### `data://schema`
Automatically provides the LLM with the database schema (tables, columns, types) so it can write accurate queries without guessing.

## ğŸ“ Structured Logging

All operations emit structured JSON logs to stderr:

```json
{"timestamp": "2026-02-21 12:00:00", "level": "INFO", "module": "src.db", "message": "db_initialized", "source": "data/sample.db", "type": ".db"}
{"timestamp": "2026-02-21 12:00:01", "level": "INFO", "module": "src.db", "message": "query_executed", "rows": 10, "elapsed_ms": 0.42}
{"timestamp": "2026-02-21 12:00:02", "level": "INFO", "module": "src.tools.visualize", "message": "chart_generated", "chart_type": "bar", "data_points": 4, "image_bytes": 12480}
```

Log events include:
- **`db_initialized`** â€” data source path and type
- **`query_executed`** â€” row count and execution time (ms)
- **`chart_generated`** â€” chart type, data points, image size
- **`query_rejected`** / **`query_error`** â€” blocked or failed queries

## ğŸ§ª Testing

```bash
# Run full test suite (39 tests)
pytest tests/ -v --tb=short
```

## ğŸ”„ CI/CD

The GitHub Actions pipeline (`.github/workflows/ci.yml`) runs on every push/PR:

1. **Lint** â€” `flake8` + `black --check`
2. **Test** â€” `pytest tests/ -v`
3. **Docker Build** â€” builds the image with Buildx (no push)
